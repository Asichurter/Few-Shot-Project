# 运行描述

## 第1次运行

1. 参照Resnet的实现，先使用一个大kernel_size(7)的卷积层提取特征，再输入到残差块中。
每个残差块由2个卷积层组成，之间不改变channel，也不使用池化，池化只在残差块之间使用

2. 使用Proto来组合提取到的特征，距离使用的是平方欧式距离

3. 使用4个残差块

4. 损失函数使用负对数损失函数

5. 训练集增加至60个

6. 训练周期减小至5000轮

7. 使用SGD优化

## 第2次运行

1. 训练集增加至300个

2. 使用Adam优化

3. 修改为按照验证正确率保存模型

## 第3次运行

3. 验证集增加至81个，剩余30个用于测试

4. 修正了致命错误：ResidualBlock在forward时没有正确串联

5. 在shortcut中增加了一个kernel_size=1的conv层和BN层用于适应channel的变化。
如果channel没有变化，则该层不存在，直接为x

## 第4次运行

1. 增加了残差块至6个

2. 最大训练60000次，每10000次询问是否停止 

## 第5次运行(模型被不小心覆盖了)

1. 改变了残差块的结构，使得每个残差块只有一个卷积层

2. 训练了20000轮

## 第6次运行

1. 改为Relation结构

2. 使用SGD优化，权重衰竭改为5e-4

## 第7次运行

1. 改回Adam优化，为微调训练模型

## 第8次运行

1. 由于不能收敛，因此减小到100个训练集

2. 修改了训练集的指定方式：共300个训练集，指定的训练集总数n将会从中抽取
n个类，而不是之前的单纯抽取前n个类

## 进一步的目标：

- 8.24：
重新进行Residual+Proto的网络的实验，至少训练20000轮，
尽量保证其收敛，训练集和测试集拉满。同时尝试对其进行fine-tuning

