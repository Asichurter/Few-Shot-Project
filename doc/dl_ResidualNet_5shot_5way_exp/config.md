# 运行描述

## 第1次运行

1. 参照Resnet的实现，先使用一个大kernel_size(7)的卷积层提取特征，再输入到残差块中。
每个残差块由2个卷积层组成，之间不改变channel，也不使用池化，池化只在残差块之间使用

2. 使用Proto来组合提取到的特征，距离使用的是平方欧式距离

3. 使用4个残差块

4. 损失函数使用负对数损失函数

5. 训练集增加至60个

6. 训练周期减小至5000轮

7. 使用SGD优化

## 第2次运行

1. 训练集增加至300个

2. 使用Adam优化

3. 修改为按照验证正确率保存模型

## 第3次运行

3. 验证集增加至81个，剩余30个用于测试

4. 修正了致命错误：ResidualBlock在forward时没有正确串联

5. 在shortcut中增加了一个kernel_size=1的conv层和BN层用于适应channel的变化。
如果channel没有变化，则该层不存在，直接为x

## 第4次运行

1. 增加了残差块至6个

2. 最大训练60000次，每10000次询问是否停止 

## 第5次运行(模型被不小心覆盖了)

1. 改变了残差块的结构，使得每个残差块只有一个卷积层

2. 训练了20000轮

## 第6次运行

1. 改为Relation结构

2. 使用SGD优化，权重衰竭改为5e-4

## 第7次运行

1. 改回Adam优化，为微调训练模型

## 第8次运行

1. 由于不能收敛，因此减小到100个训练集

2. 修改了训练集的指定方式：共300个训练集，指定的训练集总数n将会从中抽取
n个类，而不是之前的单纯抽取前n个类

## 第9次运行

1. 改为Proto

2. 每训练5000轮就询问一次，目标是训练至收敛

3. 学习率2000轮下降一次 

## 第10次运行

1. 将残差块数量提升至6

2. 使用论文中的初始化方式

## 第11次运行

1. 增加了embed以后类内向量的方差与类间原型向量的方差作为损失函数的一部分加入
到训练过程中，其中类间方差的系数和类内方差的系数均为0.01

## 第12次运行

1. 采样器修改为从N个样本中随机抽取k个作为sample，qk个作为query，而不是之前
的固定的前k个作为sample，后qk个作为query

2. 本来是训练20000轮的，但是没有收敛，因此继续训练。继续训练时，不在变化学习率，
而是使用固定的1e-5

## 第13次运行

1. 测试在使用随机采样器的时候，没有添加类内方差和类间方差训练的时候的模型

## 第14次运行

1. 加入了卷积后的特征转换Transformer

2. 改为100轮验证一次，每次验证50轮；学习率改为1000轮减小一次

# 第15次运行

1. 删去了Transformer，重新测试带类间方差和类内方差损失的优化方式

# 第16次运行

1. 重新测试单纯为负对数损失优化方式的效果，与15次作对比

# 第17次运行

1. 测试在数据集变为随机抽样后，Relation的性能

# 第18次运行

1. 修正了数据集，使得子类内部的数据不再出现是别的子类的情况

2. 恢复使用类间和类内方差添加到损失中

# 第19次运行

1. 与第18次作对比，测试没有添加方差的情况下训练

# 第20次运行

1. 修改了 超参数，增大了方差的系数：从0.01 减小到0.001

2. 学习率2000轮下降一次

## 进一步的目标：

- 8.24：
重新进行Residual+Proto的网络的实验，至少训练20000轮，
尽量保证其收敛，训练集和测试集拉满。同时尝试对其进行fine-tuning

- 8.25
由于从嵌入向量后的MDS分析，发现同类别间的向量没有聚类倾向，
同时原型向量之间分离的不是很充分。因此考虑：1.在损失函数中增加类间
原型的距离。2.在损失函数中加入类内查询集样本距离。可以参照LDA的训练
模式，最大化类间散度，最小化类内散度
